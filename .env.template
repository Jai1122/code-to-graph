# ===== CodeToGraph Configuration Template =====
# Copy this file to .env and customize for your environment
# This file is safe to commit - it contains no actual secrets

# ===== Neo4j Database Configuration =====
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=REPLACE_WITH_YOUR_PASSWORD
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_LIFETIME=3600
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# ===== LLM Provider Configuration =====
# Choose your provider: ollama, vllm, openai
LLM_PROVIDER=ollama

# OLLAMA Configuration (for local inference)
LLM_OLLAMA_BASE_URL=http://localhost:11434
LLM_OLLAMA_MODEL=qwen3:1.7b

# VLLM Configuration (for remote inference with API key)
LLM_VLLM_BASE_URL=https://your-vllm-endpoint.com
LLM_VLLM_API_KEY=REPLACE_WITH_YOUR_API_KEY
LLM_VLLM_MODEL=/app/models/qwen3:14b

# OpenAI Configuration (alternative provider)
LLM_OPENAI_API_KEY=REPLACE_WITH_YOUR_OPENAI_KEY
LLM_OPENAI_MODEL=gpt-4o

# General LLM Settings
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.1
LLM_TIMEOUT=120
LLM_ENABLE_CACHING=true
LLM_CACHE_TTL=3600

# ===== Processing Configuration =====
PROCESSING_CHUNK_STRATEGY=hybrid
PROCESSING_MAX_CHUNK_SIZE=100
PROCESSING_MAX_MEMORY_GB=16
PROCESSING_ENABLE_TREE_SITTER=true
PROCESSING_ENABLE_JOERN=true
PROCESSING_JOERN_HEAP_SIZE=8G
PROCESSING_ENABLE_INCREMENTAL=true
PROCESSING_TRACK_FILE_HASHES=true

# ===== Visualization Configuration =====
VIZ_HOST=localhost
VIZ_PORT=8080
VIZ_DEBUG=false
VIZ_MAX_NODES_PER_VIEW=1000
VIZ_DEFAULT_LAYOUT=force
VIZ_ENABLE_PHYSICS=true
VIZ_ENABLE_DRILL_DOWN=true

# ===== Application Settings =====
APP_NAME=CodeToGraph
VERSION=0.1.0
DEBUG=false
LOG_LEVEL=INFO
LOG_FORMAT="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"

# ===== Directory Paths =====
# These will be created automatically if they don't exist
DATA_DIR=./data
CACHE_DIR=./cache
LOGS_DIR=./logs
TEMP_DIR=./tmp